{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapeando tango (1 de varios)\n",
    "\n",
    "## Bajando información de los \"creadores\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La idea de este post es mostrar una forma bien sencilla y rápida de webscraping. Particularmente, estamos trabajando en un proyecto para hacer análisis de letras de tango con NLP. En ese sentido, los datos relativos a los letristas, compositores, cantores y demás figuras dentro del tango probablemente resulten relevantes.\n",
    "\n",
    "Este notebook muestra una implementación de una función simple de scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La fuente: [Todo Tango](http://www.todotango.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El sitio [Todo Tango](http://www.todotango.com/) tiene una gran cantidad de información acerca de letras, compositores, músicos, cantores y letristas de tango. Pero, además, tiene una estructura que la hace bastante fácil de scrapear (shhhhh...). \n",
    "\n",
    "Básicamente, podemos acceder al link \"maestro\" que contiene todos los links de los llamados \"creadores\" (es decir, todas las figuras que nos interesan). \n",
    "\n",
    "* http://www.todotango.com/creadores/\n",
    "\n",
    "Ahí podemos ver que hay varias solapas por tipo de \"creador\". Pero existe una que es \"Todos\". \n",
    "\n",
    "Luego, se nota que hay una solapa por cada letra del apellido. Pero, una vez más, hay una solapa [\"\\*\"] que contiene todos los links de los creadores.\n",
    "\n",
    "* http://www.todotango.com/creadores/listar/all/all/\n",
    "\n",
    "Entonces, lo que vamos a tener que hacer es lo siguiente:\n",
    "\n",
    "1. scrapear todos los links que van a cada una de las páginas de los creadores\n",
    "2. entrar a todas las páginas de los creadores y scrapear la información que nos interesa\n",
    "\n",
    "Lo bueno es que la información adentro de cada una de las páginas está bastante estructurada y es bastante fácil de scrapear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cómo hacerlo..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Primero, importamos los paquetes a utilizar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from numpy import random\n",
    "import multiprocessing as mp\n",
    "import json, codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Usando [`urlib2`](https://docs.python.org/2/library/urllib2.html) y [`BeautifulSoup`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#) generamos una lista que contiene todos los links en la página de \"los creadores\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = urllib2.urlopen('http://www.todotango.com/creadores/listar/all/all/').read()\n",
    "index_soup = soup(index,\"html.parser\")\n",
    "index_link = []\n",
    "for link in index_soup.find_all('a'):\n",
    "    index_link.append(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) Nos quedamos con aquellos que corresponden a links de creadores (excluimos links a otras páginas, Twitter, etc.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "links_creadores = index_link[51:4676]\n",
    "del links_creadores[1894]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4) Armamos una función que toma como argumentos la lista con links y un nombre de archivo.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrap_creadores(lista, save_file):\n",
    "    creadores = []\n",
    "    count = 0\n",
    "    for i in lista:\n",
    "        count += 1\n",
    "        print count, \n",
    "        r = urllib2.urlopen(i).read()\n",
    "        page_soup = soup(r,\"html.parser\") \n",
    "        name = page_soup.findAll('span', attrs={'id':'main_fichacreador1_encabezado1_lbl_NombreCompleto'})[0].get_text()\n",
    "        cat = page_soup.findAll('span', attrs={'id':'main_fichacreador1_encabezado1_lbl_Categoria'})[0].get_text()\n",
    "        fecha = page_soup.findAll('span', attrs={'id':'main_fichacreador1_encabezado1_lbl_Fechas'})[0].get_text()\n",
    "        lugar = page_soup.findAll('span', attrs={'id':'main_fichacreador1_encabezado1_lbl_LugarNacimiento'})[0].get_text()\n",
    "        creadores.append((i, name, cat, fecha,lugar))\n",
    "        with open(save_file, 'wb') as f:\n",
    "            json.dump(creadores, codecs.getwriter('utf-8')(f), ensure_ascii=False)\n",
    "    return creadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Explicación (breve) de la función:**\n",
    "\n",
    "4.1) Esta función recorre la lista de links \n",
    "    \n",
    "    for i in lista:\n",
    "\n",
    "4.2) descarga y parsea cada uno de los códigos html de cada página de los creadores\n",
    "\n",
    "    r = urllib2.urlopen(i).read()\n",
    "    page_soup = soup(r,\"html.parser\")\n",
    "    \n",
    "4.3) identifica la información biográfica relevante y la guarda en una variable\n",
    "\n",
    "    name = page_soup.findAll('span', attrs={'id':'main_fichacreador1_encabezado1_lbl_NombreCompleto'})[0].get_text()\n",
    "    cat = page_soup.findAll('span', attrs={'id':'main_fichacreador1_encabezado1_lbl_Categoria'})[0].get_text()\n",
    "    fecha = page_soup.findAll('span', attrs={'id':'main_fichacreador1_encabezado1_lbl_Fechas'})[0].get_text()\n",
    "    lugar = page_soup.findAll('span', attrs={'id':'main_fichacreador1_encabezado1_lbl_LugarNacimiento'})[0].get_text()\n",
    "    \n",
    "4.4) guarda todo en una lista generada previamente...\n",
    "     \n",
    "    creadores.append((i, name, cat, fecha,lugar))\n",
    "         \n",
    "4.5) y, finalmente, escribe un archivo `.txt` con la lista\n",
    "\n",
    "    with open(save_file, 'wb') as f:\n",
    "        json.dump(creadores, codecs.getwriter('utf-8')(f), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158"
     ]
    }
   ],
   "source": [
    "creador = scrap_creadores(links_creadores, \"creadores.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
